---
title: "NC State Senate Facebook Posts"
author: "Ryan Wesslen"
date: "Jan 1, 2017"
output: html_document
---
  
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

### Pre-processing & Exploration

#### Counts

Let's read in two datasets: the senators and their public Facebook posts. The posts were collected through Facebook's public API via R's `Rfacebook` package. You can view the pseudo code used to pull this data from the file in the code folder of the GitHub directory.

```{r}
fbData <- read.csv("~/Dropbox (UNC Charlotte)/NCStateSenateFacebook/data/facebookPosts.csv", stringsAsFactors = F)
Senators <- read.csv("~/Dropbox (UNC Charlotte)/NCStateSenateFacebook/data/NCSenate.csv", stringsAsFactors = F)
```

Now let's merge the two datasets and then filter to only Facebook posts from 2015 and 2016. 

Note -- this data was pulled on 12-23 and excludes any posts that have been posted since that date. Also, the number of responses (likes, comments, shares) were as of 12-23 and could have changed since that date.

```{r}
library(dplyr)
fbData <- merge(fbData, Senators, by.x = "from_id", by.y = "Facebook")
model <- subset(fbData, substr(created_time,1,4) %in% c("2015","2016"))
```

This leaves 7,813 facebook posts.

Let's explore the number of posts and the average number of likes, comments, shares by party.

```{r}
model %>% 
  group_by(Party) %>% 
  summarise(AvgLikes = mean(likes_count), ComCount = mean(comments_count), ShCount = mean(shares_count), Count=n())
```

So we have about the same number of posts for each party -- however, notice that, on average, Democrat state senators have a larger response by Facebook users (more likes, more comments, and more shares on average).

But there are different types of Facebook posts: links, photo, videos, event notifications and regular posts (status or note).

How does the number of posts differ by the type of post?

```{r}
table(model$Party, model$type)
```

The count is fairly even across party but it's clear there are two main types of posts: links (42%) and photos (40%). Statuses, videos and events make up the remaining (about 18%).

How have these trends changed over time? 

#### Counts over time

First, let's consider the total number of posts by plotting it by party.

```{r fig.height=6, fig.width=8}
library(ggplot2); library(scales)

plotData <- model %>% 
  group_by(Date=substr(created_time,1,7), Party) %>% 
  summarise(Posts=n(), Comments=sum(comments_count), Shares=sum(shares_count), Likes=sum(likes_count))

plotData$Month <- as.Date(paste0(plotData$Date,"-01"))

ggplot(data=plotData, aes(x=Month, y=Posts, group=Party, colour=Party)) +
  geom_line() + 
  geom_point() +
  xlab("Month") + ylab("Number of Facebook Posts") + # Set axis labels
  ggtitle("Number of Public Facebook Posts by NC State Senators \n By Political Party")  +   # Set title
  scale_color_manual(breaks = c("Democratic","Republican"), values=c("blue","red")) + # Manually change to blue and red
  scale_x_date(labels = date_format("%m-%Y")) +
  theme(text = element_text(size=18)) + 
  theme(legend.position = c(0.2,0.6), legend.background = element_rect(colour = "black")) +     
  theme(plot.margin = unit(c(1,1,1,1), "cm")) +
  theme(axis.title.x=element_text(margin=margin(20,0,0,0))) +
  theme(axis.title.y=element_text(angle=90, margin=margin(0,20,0,0))) +
  theme(plot.title=element_text(size=20, margin=margin(0,0,20,0))) 
```

In 2015, Democrats were more active on Facebook while Republicans became much more active on Facebook during the three months before the election with more than double the number of posts.

How does this count over time change relative to the type of post (link, photo or all other types)?

```{r fig.height=6, fig.width=8}
model$adjtype <- ifelse(model$type %in% c("link","photo"),model$type,"status or video")

plotData <- model %>% 
  group_by(Date=substr(created_time,1,7), Party, adjtype) %>% 
  summarise(Posts=n(), Comments=sum(comments_count), Shares=sum(shares_count), Likes=sum(likes_count))

plotData$Month <- as.Date(paste0(plotData$Date,"-01"))

ggplot(data=plotData, aes(x=Month, y=Posts, group=Party, colour=Party)) +
  geom_line() + 
  geom_point() +
  xlab("Month") + ylab("Number of Facebook Posts") + # Set axis labels
  ggtitle("Number of Public Facebook Posts by NC State Senators \n By Political Party")  +   # Set title
  scale_color_manual(breaks = c("Democratic","Republican"), values=c("blue","red")) + # Manually change to blue and red
  facet_grid(adjtype ~ .) +
  scale_x_date(labels = date_format("%m-%Y")) +
  theme(text = element_text(size=18)) + 
  theme(legend.position = "bottom", legend.background = element_rect(colour = "black")) +     
  theme(plot.margin = unit(c(1,1,1,1), "cm")) +
  theme(axis.title.x=element_text(margin=margin(20,0,0,0))) +
  theme(axis.title.y=element_text(angle=90, margin=margin(0,20,0,0))) +
  theme(plot.title=element_text(size=20, margin=margin(0,0,20,0))) 
```

It's relatively similar to the totals. One difference is that a lot of the recent Republican growth was in photos (and somewhat statuses and videos). 

For our purposes, I'm interested in posts that include text -- so let's count how many of the 7,813 posts do not include any text (e.g. only a photo or video).

```{r}
nrow(subset(model, is.na(message)))
```

Let's drop messages (for now) that do not include any text. 

```{r}
model <- subset(model, !(is.na(message)))
```

Let's summarise the count of posts and on average how long the posts are for each senator (by word count).

```{r}
library(quanteda)

agg <- model %>% 
        group_by(Member, Party) %>% 
        summarise(Count =n(), AvgWords = mean(ntoken(message)), AvgLikes = mean(likes_count), AvgComments = mean(comments_count), AvgShares = mean(shares_count)) %>% 
        arrange(desc(Count))

head(agg, 10)
```

Let's reorder the count three times: by the highest number of average likes, comments and shares.

```{r}
head(agg[order(agg$AvgLikes, decreasing = T),],10)

head(agg[order(agg$AvgComments, decreasing = T),],10)

head(agg[order(agg$AvgShares, decreasing = T),],10)
```

What's clear is that from a social response, there are two major state senators: Jeff Jackson (D) and Phil Berger (R). This isn't so surprising. Both have the highest number of Facebook likes on their pages (Berger has over 120k and Jackson has nearly 50k). This is also similar to the number of Twitter followers (see [my earlier post](https://wesslen.github.io/social%20media/nc-state-senator-twitter-network/) on the state senators' Twitter follower network). 

Another observation is that Jeff Jackson gets nearly double the number of likes (on average) than Phil Berger, yet has less than half the number of followers. Alternatively, Phil Berger gets a slightly higher number of comments and shares than Jeff Jackson on average. It's interesting to compare likes and comments/shares as likes are a very simple response while comments and shares typically show a little more engagement as they elicit either a pseudo-endorsement (shares) or a verbal response (comment). My interpretation is that Senator Berger's posts elicit a bit stronger sentiment (not clear positive or negative) while Senator Jackson's posts elicit a larger "passive" response.

Through the public API, I was also able to pull all of the comments and like data (name/facebook ID) to identify who are the Facebook users who have commented or liked the posts of all the senators. If I get a chance, I'll post a future post analyzing who are the users that are responding to the posts.

What is somewhat suprising is how little a response nearly all of the remaining state senators typically have. In fact, on average Senators Berger and Jackson get well more than the number of responses (likes, comments or shares) than all other NC state senators. The main message of this: Senators Berger and Jackson are the party leaders on Facebook for each's respective party. This isn't surprising given that Senator Berger is the Senate (and party) leader; however, it's worth noting that Senator Jackson is a relatively new senator who only joined the Senate in May 2014 to fill an open seat. 

### Automated Text Analysis

Let's now explore the content of the posts to determine what are the most salient topics discussed and analyze how the topics differ by political party.

To analyze, I'll use the `quanteda` package. 

```{r}
myCorpus <- corpus(model$message)
docvars(myCorpus, "Party") <- as.factor(model$Party)
docvars(myCorpus, "Time") <- as.integer(as.factor(substr(model$created_time,1,7)))

stpWords <- c("t.co", "https", "rt", "amp", "http", "t.c", "can", "u", "bit.ly","fb.me","facebook","posted", "also", "first", "second", "third", "one", "two", "three", "tt", "just", "said", "need", "going", "well", "like", "will", "www.gunnforncsenate.com","269eee9031f118516beeaa7c1","www.newsobserver.com","www.youtube.com", "NA")

dfm <- dfm(myCorpus,
           ignoredFeatures = c(stopwords("english"), stpWords),
           ngrams=c(1,2))
```

Let's explore two word clouds -- the first showing the raw frequencies (word size = frequency the word is mentioned) and by the [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) weighting (word size ~= importance of the word as measured by TF-IDF).

```{r fig.height=8, fig.width=8}
vdfm <- trim(dfm, minCount = 10, minDoc = 5)

library(RColorBrewer)
plot(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250, main = "Count")
plot(tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250, main = "TF-IDF")
```

What's clear is that there are a lot of North Carolina-related topics being discussed. But word clouds does not reveal the patterns of words being used together (i.e. topics).

As an exploratory exercise, we can use word clustering to identify words that tend to be used similarly (in the same post).

```{r fig.width=8, fig.height=6}
par(mfrow = c(1,1),mar = c(4, 4, 4, 4))
twdfm <- trim(vdfm, minCount=50)
wordDfm <- sort(weight(twdfm, "tfidf"))
wordDfm <- t(wordDfm)[1:50,]  # because transposed
wordDistMat <- dist(wordDfm)
wordCluster <- hclust(wordDistMat)
plot(wordCluster, xlab="", main="TF-IDF Frequency weighting")
```

This is helpful and the cluster of words are generally intuitive (e.g. Trump/Clinton, Charlotte/HB2, Governor/McCrory). While this is a good "gut" check on the data, such analysis is insufficient at measuring and identifying the topics. 

To analyze the topics, I'll use [topic modeling](https://en.wikipedia.org/wiki/Topic_model), a framework of machine learning algorithms to analyze text through statistical patterns within large collection of documents (in our case, Facebook posts). Topic models are a class of unsupervised machine learning algorithms that, on their own, identify clusters of words that co-occur within the documents that we can interpret as the topics. Instead of knowing beforehand what are the topics discussed (e.g. dictionary-based/supervised methods), topic models allows the data to speak for itself by identifying the topics on its own. 

I'll use a cutting-edge extension of the topic models called Structural Topic Modeling (or STM) that will allow me to estimate, with statistical confidence, the effect political party has on the occurrence of each topic identified.

### Structural Topic Model

Let's first convert the data to a `stm`-based data structure.

```{r fig.height=6, fig.width=8}
library(stm)

# use quanteda converter to convert our Dfm
stmdfm <- convert(dfm, to = "stm", docvars = docvars(myCorpus))

plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))

out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 3)
```

First, I'm going to run an analysis to give an estimate for the number of appropriate topics.

```{r}
#K<-c(10,20,30,50,75,100)
#kresult <- searchK(out$documents, out$vocab, K, prevalence=~Party + s(Time), data = out$meta, init.type = "Spectral")
#plot(kresult)
```

From these results, I'm going to run a 30-topic model for simplicity. 

```{r results="hide"}
k <- 40

stmFit <- stm(out$documents, out$vocab, K = k, prevalence =~ Party + s(Time),
                 max.em.its = 200, data = out$meta, init.type = "Spectral", seed = 300)
```

Let's explore the size of the topics by their topic proportions.

```{r fig.height=8}
plot.STM(stmFit, type = "summary", xlim = c(0,.13), n = 5, 
         main = "NC State Senator's Topics on Facebook", text.cex = 0.8)
```

Using some analysis, I'

```{r results="hide"}
# topic <- data.frame( 
#   topicnames = c("Coal Ash",
#                  "Health Care",
#                  "Voting",
#                  "Conservative Values",
#                  "Support Campaign",
#                  "HB2",
#                  "Voting Rights",
#                  "Disaster Services",
#                  "Religious Freedom",
#                  "Legislation",
#                  
#                  "Community Events",
#                  "Constituent Thanks",
#                  "Teachers",
#                  "Bathroom Safety",
#                  "Law Enforcement",
#                  "Education Funding",
#                  "North Carolina",
#                  "Economy",
#                  "Berger-Newton",
#                  "Women's Issues",
#                  
#                  "Governor Election",
#                  "Hillary Clinton",
#                  "State Senate",
#                  "Public Opinion",
#                  "Redistricting",
#                  "Taxes",
#                  "Partisan Issue",
#                  "Gun Violence",
#                  "Donald Trump",
#                  "Civil Rights"),
#   TopicNumber = 1:k)
```

```{r}
#shortdoc <- substr(subset(fbData$ID,documents$meta$ID,1,200)
#thoughts3 <- findThoughts(stmFit, texts = shortdoc, n = 2, topics = 2)

par(mfrow = c(4,2),mar = c(1, 1, 2, 1))
for (i in 1:k){
  plot.STM(stmFit, type = "labels", n = 15, topics = i, #main = paste0(topic$topicnames[i]," - Raw Probabilities")
           width = 55)
  plot.STM(stmFit, type = "labels", n = 15, topics = i, #main = paste0(topic$topicnames[i]," - FREX"), 
           labeltype = "frex", width = 55)
}
```

```{r fig.height=6}
par(mfrow = c(1,1),mar = c(2, 2, 2, 2))
topicQuality(stmFit, documents = out$documents, main = "Topic Interpretability: Exclusivity and Semantic Coherence")
```

```{r fig.height=8, fig.width=7}
prep <- estimateEffect(1:k ~ Party + s(Time), stmFit, meta = out$meta, uncertainty = "Global")

#rank = c(12,4,14,19,29,9,23,18,8,26,16,17,21,27,15,22,2,5,20,28,25,7,30,1,24,13,3,10,11,6)
#topic_name <- topic[rank,]

par(mfrow = c(1,1),mar = c(6, 6, 4, 4))
plot.estimateEffect(prep, "Party", method = "difference", cov.value1 = "Democratic", cov.value2 = "Republican", 
                    #topics = topic_name$TopicNumber,
                    verbose.labels = F, ylab = "Expected Difference in Topic Probability by Party", 
                    #labeltype = "custom",
                    xlab = "More Likely Republican                                 Not Significant                                 More Likely Democratic",
                    #custom.labels  = topic_name$topicnames, 
                    main = "Effect of Party on Topic Prevelance of Facebook Posts for NC State Senators",
                    xlim = c(-.1,0.1))
```

```{r}
# time
par(mfrow = c(2,2),mar = c(4,4,2,2))
for (i in 1:k){
  plot.estimateEffect(prep, "Time", method = "continuous", topics = i, model = z,  main = topic$topicnames[i],
                      printlegend = FALSE, xaxt = "n", ylab = "Exp. Topic Prob", xlab = "Time (Month)"
                      , ylim = c(-0.01,0.15)
                      )
}
```

#### Comparing Campaign-related Topics

```{r}
# interesting comparisons
plot.STM(stmFit, type = "perspectives", topics = c(6,14))
```

#### Comparing HB2-related Topics

```{r}
# interesting comparisons
plot.STM(stmFit, type = "perspectives", topics = c(6,14))
```

### Topic Correlations

```{r}
library(igraph); library(visNetwork)
par(mfrow = c(1,1))

mod.out.corr <- topicCorr(stmFit, cutoff = .04)
plot(mod.out.corr)

links2 <- as.matrix(mod.out.corr$posadj)
net2 <- graph_from_adjacency_matrix(links2, mode = "undirected")
table(V(net2)$type)
net2 <- simplify(net2, remove.multiple = F, remove.loops = T) 

links <- as_data_frame(net2, what="edges")
nodes <- as_data_frame(net2, what="vertices")

#visNetwork edits
nodes$shape <- "dot"  
nodes$shadow <- TRUE # Nodes will drop shadow
#nodes$title <- nodes$media # Text on click
nodes$label <- paste0("Topic ",1:k) #topic_name$topicnames # Node label
#nodes$size <- nodes$audience.size # Node size
nodes$borderWidth <- 2 # Node border width

#nodes$color.background <- c("slategrey", "tomato", "gold")[nodes$media.type]
nodes$color.border <- "black"
nodes$color.highlight.background <- "orange"
nodes$color.highlight.border <- "darkred"
nodes$id <- 1:k

visNetwork(nodes, links, width="100%",  height="500px",main="Topic Correlations")
```